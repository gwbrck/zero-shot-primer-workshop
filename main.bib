@online{bachlComputationalTextAnalysis2024,
  title = {Computational Text Analysis},
  author = {Bachl, Marko and Scharkow, Michael},
  date = {2024-10-02},
  eprinttype = {Open Science Framework},
  doi = {10.31219/osf.io/3yhu8},
  url = {https://osf.io/3yhu8_v1},
  urldate = {2025-11-26},
  abstract = {Computational text analysis (CTA) comprises techniques for measuring the content of texts with the help of computer algorithms. The methods are discussed under various labels, such as text-as-data, automated content analysis, natural language processing, or text mining. The defining characteristic of a CTA technique is that once it is initially configured, the computational system performs the measurements independently without requiring any manual intervention or effort. The strength of CTA lies in its scalability, enabling the measurement of characteristics across vast amounts of text. As a result, CTA has seen widespread application in communication, related social sciences, and the digital humanities, with the increasing availability of digital or digitized, machine-readable texts.We start this chapter with an overview of the historical development of CTA. We then systematize CTA along two dimensions: the representations of texts for the computational analysis and the supervision of the measurement process. While doing so, we provide some examples of popular techniques. The chapter ends with an outlook into the near future.},
  pubstate = {prepublished}
}

@article{gielensGoodbyeHumanAnnotators2025,
  title = {Goodbye Human Annotators? {{Content}} Analysis of Social Policy Debates Using {{ChatGPT}}},
  shorttitle = {Goodbye Human Annotators?},
  author = {Gielens, Erwin and Sowula, Jakub and Leifeld, Philip},
  date = {2025-01-03},
  journaltitle = {Journal of Social Policy},
  shortjournal = {J. Soc. Pol.},
  pages = {1--20},
  issn = {0047-2794, 1469-7823},
  doi = {10.1017/S0047279424000382},
  url = {https://www.cambridge.org/core/product/identifier/S0047279424000382/type/journal_article},
  urldate = {2025-11-21},
  abstract = {Abstract             Content analysis is a valuable tool for analysing policy discourse, but annotation by humans is costly and time consuming. ChatGPT is a potentially valuable tool to partially automate content analysis for policy debates, largely replacing human annotators. We evaluate ChatGPT’s ability to classify documents using pre-defined argument descriptions, comparing its performance with human annotators for two policy debates: the Universal Basic Income debate on Dutch Twitter (2014–2016) and the pension reforms debate in German newspapers (1993–2001). We use the API (GPT-4 Turbo) and user interface version (GPT-4) and evaluate multiple performance metrics (accuracy, precision and recall). ChatGPT is highly reliable and accurate in classifying pre-defined arguments across datasets. However, precision and recall are much lower, and vary strongly between arguments. These results hold for both datasets, despite differences in language and media type. Moreover, the cut-off method proposed in this paper may aid researchers in navigating the trade-off between detection and noise. Overall, we do not (yet) recommend a blind application of ChatGPT to classify arguments in policy debates. Those interested in adopting this tool should manually validate bot classifications before using them in further analyses. At least for now, human annotators are here to stay.},
  langid = {english},
  file = {/Users/gwbrck/Zotero/storage/S2L6LE7T/Gielens et al. - 2025 - Goodbye human annotators Content analysis of social policy debates using ChatGPT.pdf}
}

@article{gilardi-alizadeh-ea_2023_chatg,
  title = {{{ChatGPT}} Outperforms Crowd Workers for Text-Annotation Tasks},
  author = {Gilardi, Fabrizio and Alizadeh, Meysam and Kubli, Maël},
  date = {2023-07-25},
  journaltitle = {Proceedings of the National Academy of Sciences},
  shortjournal = {Proc. Natl. Acad. Sci. U.S.A.},
  volume = {120},
  number = {30},
  pages = {e2305016120},
  issn = {0027-8424, 1091-6490},
  doi = {10.1073/pnas.2305016120},
  url = {https://pnas.org/doi/10.1073/pnas.2305016120},
  urldate = {2024-01-17},
  langid = {english},
  annotation = {Read\_Status: New\\
Read\_Status\_Date: 2025-04-06T19:20:50.708Z},
  file = {/Users/gwbrck/Zotero/storage/FQVM5YMN/Gilardi et al. - 2023 - ChatGPT outperforms crowd workers for text-annotation tasks.pdf}
}

@online{guptaBiasRunsDeep2023,
  title = {Bias {{Runs Deep}}: {{Implicit Reasoning Biases}} in {{Persona-Assigned LLMs}}},
  shorttitle = {Bias {{Runs Deep}}},
  author = {Gupta, Shashank and Shrivastava, Vaishnavi and Deshpande, Ameet and Kalyan, Ashwin and Clark, Peter and Sabharwal, Ashish and Khot, Tushar},
  date = {2023},
  doi = {10.48550/ARXIV.2311.04892},
  url = {https://arxiv.org/abs/2311.04892},
  urldate = {2025-11-26},
  abstract = {Recent works have showcased the ability of LLMs to embody diverse personas in their responses, exemplified by prompts like 'You are Yoda. Explain the Theory of Relativity.' While this ability allows personalization of LLMs and enables human behavior simulation, its effect on LLMs' capabilities remains unclear. To fill this gap, we present the first extensive study of the unintended side-effects of persona assignment on the ability of LLMs to perform basic reasoning tasks. Our study covers 24 reasoning datasets, 4 LLMs, and 19 diverse personas (e.g. an Asian person) spanning 5 socio-demographic groups. Our experiments unveil that LLMs harbor deep rooted bias against various socio-demographics underneath a veneer of fairness. While they overtly reject stereotypes when explicitly asked ('Are Black people less skilled at mathematics?'), they manifest stereotypical and erroneous presumptions when asked to answer questions while adopting a persona. These can be observed as abstentions in responses, e.g., 'As a Black person, I can't answer this question as it requires math knowledge', and generally result in a substantial performance drop. Our experiments with ChatGPT-3.5 show that this bias is ubiquitous - 80\% of our personas demonstrate bias; it is significant - some datasets show performance drops of 70\%+; and can be especially harmful for certain groups - some personas suffer statistically significant drops on 80\%+ of the datasets. Overall, all 4 LLMs exhibit this bias to varying extents, with GPT-4-Turbo showing the least but still a problematic amount of bias (evident in 42\% of the personas). Further analysis shows that these persona-induced errors can be hard-to-discern and hard-to-avoid. Our findings serve as a cautionary tale that the practice of assigning personas to LLMs - a trend on the rise - can surface their deep-rooted biases and have unforeseeable and detrimental side-effects.},
  pubstate = {prepublished},
  version = {2},
  keywords = {Computation and Language (cs.CL),FOS: Computer and information sciences}
}

@article{heseltineLargeLanguageModels2024,
  title = {Large Language Models as a Substitute for Human Experts in Annotating Political Text},
  author = {Heseltine, Michael and Clemm Von Hohenberg, Bernhard},
  date = {2024-01},
  journaltitle = {Research \& Politics},
  shortjournal = {Research \& Politics},
  volume = {11},
  number = {1},
  pages = {20531680241236239},
  issn = {2053-1680, 2053-1680},
  doi = {10.1177/20531680241236239},
  url = {https://journals.sagepub.com/doi/10.1177/20531680241236239},
  urldate = {2025-11-21},
  abstract = {Large-scale text analysis has grown rapidly as a method in political science and beyond. To date, text-as-data methods rely on large volumes of human-annotated training examples, which place a premium on researcher resources. However, advances in large language models (LLMs) may make automated annotation increasingly viable. This paper tests the performance of GPT-4 across a range of scenarios relevant for analysis of political text. We compare GPT-4 coding with human expert coding of tweets and news articles across four variables (whether text is political, its negativity, its sentiment, and its ideology) and across four countries (the United States, Chile, Germany, and Italy). GPT-4 coding is highly accurate, especially for shorter texts such as tweets, correctly classifying texts up to 95\% of the time. Performance drops for longer news articles, and very slightly for non-English text. We introduce a ‘hybrid’ coding approach, in which disagreements of multiple GPT-4 runs are adjudicated by a human expert, which boosts accuracy. Finally, we explore downstream effects, finding that transformer models trained on hand-coded or GPT-4-coded data yield almost identical outcomes. Our results suggest that LLM-assisted coding is a viable and cost-efficient approach, although consideration should be given to task complexity.},
  langid = {english},
  file = {/Users/gwbrck/Zotero/storage/9K93XURJ/Heseltine und Clemm Von Hohenberg - 2024 - Large language models as a substitute for human experts in annotating political text.pdf}
}

@article{kjellTextpackageRpackageAnalyzing2023,
  title = {The Text-Package: {{An R-package}} for Analyzing and Visualizing Human Language Using Natural Language Processing and Transformers.},
  shorttitle = {The Text-Package},
  author = {Kjell, Oscar and Giorgi, Salvatore and Schwartz, H. Andrew},
  date = {2023-12},
  journaltitle = {Psychological Methods},
  shortjournal = {Psychological Methods},
  volume = {28},
  number = {6},
  pages = {1478--1498},
  issn = {1939-1463, 1082-989X},
  doi = {10.1037/met0000542},
  url = {https://doi.apa.org/doi/10.1037/met0000542},
  urldate = {2025-11-26},
  langid = {english}
}

@article{kristensen-mclachlanAreChatbotsReliable2025,
  title = {Are Chatbots Reliable Text Annotators? {{Sometimes}}},
  shorttitle = {Are Chatbots Reliable Text Annotators?},
  author = {Kristensen-McLachlan, Ross Deans and Canavan, Miceal and Kárdos, Marton and Jacobsen, Mia and Aarøe, Lene},
  editor = {Gelfand, Michele},
  date = {2025-03-27},
  journaltitle = {PNAS Nexus},
  volume = {4},
  number = {4},
  pages = {pgaf069},
  issn = {2752-6542},
  doi = {10.1093/pnasnexus/pgaf069},
  url = {https://academic.oup.com/pnasnexus/article/doi/10.1093/pnasnexus/pgaf069/8100281},
  urldate = {2025-11-24},
  abstract = {Abstract             Recent research highlights the significant potential of ChatGPT for text annotation in social science research. However, ChatGPT is a closed-source product, which has major drawbacks with regards to transparency, reproducibility, cost, and data protection. Recent advances in open-source (OS) large language models (LLMs) offer an alternative without these drawbacks. Thus, it is important to evaluate the performance of OS LLMs relative to ChatGPT and standard approaches to supervised machine learning classification. We conduct a systematic comparative evaluation of the performance of a range of OS LLMs alongside ChatGPT, using both zero- and few-shot learning as well as generic and custom prompts, with results compared with supervised classification models. Using a new dataset of tweets from US news media and focusing on simple binary text annotation tasks, we find significant variation in the performance of ChatGPT and OS models across the tasks and that the supervised classifier using DistilBERT generally outperforms both. Given the unreliable performance of ChatGPT and the significant challenges it poses to Open Science, we advise caution when using ChatGPT for substantive text annotation tasks.},
  langid = {english},
  file = {/Users/gwbrck/Zotero/storage/R96SC8A3/Kristensen-McLachlan et al. - 2025 - Are chatbots reliable text annotators Sometimes.pdf}
}

@online{masurComputationalAnalysisDigital2024,
  type = {Github-Repository},
  title = {Computational {{Analysis}} of {{Digital Communication}}},
  author = {Masur, Philipp},
  date = {2024},
  url = {https://github.com/masurp/VU_CADC},
  urldate = {2025-11-24},
  organization = {Course at Vrije Universiteit Amsterdam},
  file = {/Users/gwbrck/Zotero/storage/7EQUAMQI/CLASSIFICATION WITH WORD-EMBEDDINGS.png;/Users/gwbrck/Zotero/storage/86KBJJU3/dictonary_based_classification.png;/Users/gwbrck/Zotero/storage/KBAA4UES/masur_supervised_text_classification.png;/Users/gwbrck/Zotero/storage/L6J5SEWB/llmbased-classification.png;/Users/gwbrck/Zotero/storage/NSH78UH3/Bildschirmfoto 2025-11-24 um 14.10.24.png}
}

@online{ollionChatGPTTextAnnotation2023,
  title = {{{ChatGPT}} for {{Text Annotation}}? {{Mind}} the {{Hype}}!},
  shorttitle = {{{ChatGPT}} for {{Text Annotation}}?},
  author = {Ollion, Etienne and Shen, Rubing and Macanovic, Ana and Chatelain, Arnault},
  date = {2023-10-04},
  doi = {10.31235/osf.io/x58kn},
  url = {https://osf.io/x58kn},
  urldate = {2025-11-26},
  abstract = {In the past months, researchers have enthusiastically discussed the relevance of zero- or few-shot classifiers like ChatGPT for text annotation. Should these models prove to be performant, they would open up new continents for research, and beyond. To assess the merits and limits of this approach, we conducted a systematic literature review. Reading all the articles doing zero or few-shot text annotation in the human and social sciences, we found that these few- shot learners offer enticing, yet mixed results on text annotation tasks. The performance scores can vary widely, with some being average and some being very low. Besides, zero or few-shot models are often outperformed by models fine-tuned with human annotations. Our findings thus suggest that, to date, the evidence about their effectiveness remains partial, but also that their use raises several important questions about the reproducibility of results, about privacy and copyright issues, and about the primacy of the English language. While we definitely believe that there are numerous ways to harness this powerful technology productively, we also need to harness it without falling for the hype.},
  pubstate = {prepublished},
  file = {/Users/gwbrck/Zotero/storage/XXTPHLKR/Ollion et al. - 2023 - ChatGPT for Text Annotation Mind the Hype!.pdf}
}

@article{pilnyManualMachineAssessing2024,
  title = {From Manual to Machine: Assessing the Efficacy of Large Language Models in Content Analysis},
  shorttitle = {From Manual to Machine},
  author = {Pilny, Andrew and McAninch, Kelly and Slone, Amanda and Moore, Kelsey},
  date = {2024-03-14},
  journaltitle = {Communication Research Reports},
  shortjournal = {Communication Research Reports},
  volume = {41},
  number = {2},
  pages = {61--70},
  issn = {0882-4096, 1746-4099},
  doi = {10.1080/08824096.2024.2327547},
  url = {https://www.tandfonline.com/doi/full/10.1080/08824096.2024.2327547},
  urldate = {2025-11-21},
  langid = {english},
  file = {/Users/gwbrck/Zotero/storage/VZ7HAGHI/Pilny et al. - 2024 - From manual to machine assessing the efficacy of large language models in content analysis.pdf}
}

@article{rathjeGPTEffectiveTool2024,
  title = {{{GPT}} Is an Effective Tool for Multilingual Psychological Text Analysis},
  author = {Rathje, Steve and Mirea, Dan-Mircea and Sucholutsky, Ilia and Marjieh, Raja and Robertson, Claire E. and Van Bavel, Jay J.},
  date = {2024-08-20},
  journaltitle = {Proceedings of the National Academy of Sciences},
  shortjournal = {Proc. Natl. Acad. Sci. U.S.A.},
  volume = {121},
  number = {34},
  pages = {e2308950121},
  issn = {0027-8424, 1091-6490},
  doi = {10.1073/pnas.2308950121},
  url = {https://pnas.org/doi/10.1073/pnas.2308950121},
  urldate = {2025-11-26},
  abstract = {The social and behavioral sciences have been increasingly using automated text analysis to measure psychological constructs in text. We explore whether GPT, the large-language model (LLM) underlying the AI chatbot ChatGPT, can be used as a tool for automated psychological text analysis in several languages. Across 15 datasets (               n               = 47,925 manually annotated tweets and news headlines), we tested whether different versions of GPT (3.5 Turbo, 4, and 4 Turbo) can accurately detect psychological constructs (sentiment, discrete emotions, offensiveness, and moral foundations) across 12 languages. We found that GPT (               r               = 0.59 to 0.77) performed much better than English-language dictionary analysis (               r               = 0.20 to 0.30) at detecting psychological constructs as judged by manual annotators. GPT performed nearly as well as, and sometimes better than, several top-performing fine-tuned machine learning models. Moreover, GPT’s performance improved across successive versions of the model, particularly for lesser-spoken languages, and became less expensive. Overall, GPT may be superior to many existing methods of automated text analysis, since it achieves relatively high accuracy across many languages, requires no training data, and is easy to use with simple prompts (e.g., “is this text negative?”) and little coding experience. We provide sample code and a video tutorial for analyzing text with the GPT application programming interface. We argue that GPT and other LLMs help democratize automated text analysis by making advanced natural language processing capabilities more accessible, and may help facilitate more cross-linguistic research with understudied languages.},
  langid = {english},
  file = {/Users/gwbrck/Zotero/storage/7PBNTNIR/Rathje et al. - 2024 - GPT is an effective tool for multilingual psychological text analysis.pdf}
}

@online{reissTestingReliabilityChatGPT2023,
  title = {Testing the {{Reliability}} of {{ChatGPT}} for {{Text Annotation}} and {{Classification}}: {{A Cautionary Remark}}},
  shorttitle = {Testing the {{Reliability}} of {{ChatGPT}} for {{Text Annotation}} and {{Classification}}},
  author = {Reiss, Michael V.},
  date = {2023},
  doi = {10.48550/ARXIV.2304.11085},
  url = {https://arxiv.org/abs/2304.11085},
  urldate = {2025-11-26},
  abstract = {Recent studies have demonstrated promising potential of ChatGPT for various text annotation and classification tasks. However, ChatGPT is non-deterministic which means that, as with human coders, identical input can lead to different outputs. Given this, it seems appropriate to test the reliability of ChatGPT. Therefore, this study investigates the consistency of ChatGPT's zero-shot capabilities for text annotation and classification, focusing on different model parameters, prompt variations, and repetitions of identical inputs. Based on the real-world classification task of differentiating website texts into news and not news, results show that consistency in ChatGPT's classification output can fall short of scientific thresholds for reliability. For example, even minor wording alterations in prompts or repeating the identical input can lead to varying outputs. Although pooling outputs from multiple repetitions can improve reliability, this study advises caution when using ChatGPT for zero-shot text annotation and underscores the need for thorough validation, such as comparison against human-annotated data. The unsupervised application of ChatGPT for text annotation and classification is not recommended.},
  pubstate = {prepublished},
  version = {1},
  keywords = {Computation and Language (cs.CL),FOS: Computer and information sciences},
  file = {/Users/gwbrck/Zotero/storage/A5A2S6NR/Reiss - 2023 - Testing the Reliability of ChatGPT for Text Annotation and Classification A Cautionary Remark.pdf}
}

@online{tornbergBestPracticesText2024,
  title = {Best {{Practices}} for {{Text Annotation}} with {{Large Language Models}}},
  author = {Törnberg, Petter},
  date = {2024},
  doi = {10.48550/ARXIV.2402.05129},
  url = {https://arxiv.org/abs/2402.05129},
  urldate = {2025-11-21},
  abstract = {Large Language Models (LLMs) have ushered in a new era of text annotation, as their ease-of-use, high accuracy, and relatively low costs have meant that their use has exploded in recent months. However, the rapid growth of the field has meant that LLM-based annotation has become something of an academic Wild West: the lack of established practices and standards has led to concerns about the quality and validity of research. Researchers have warned that the ostensible simplicity of LLMs can be misleading, as they are prone to bias, misunderstandings, and unreliable results. Recognizing the transformative potential of LLMs, this paper proposes a comprehensive set of standards and best practices for their reliable, reproducible, and ethical use. These guidelines span critical areas such as model selection, prompt engineering, structured prompting, prompt stability analysis, rigorous model validation, and the consideration of ethical and legal implications. The paper emphasizes the need for a structured, directed, and formalized approach to using LLMs, aiming to ensure the integrity and robustness of text annotation practices, and advocates for a nuanced and critical engagement with LLMs in social scientific research.},
  pubstate = {prepublished},
  version = {1},
  keywords = {Computation and Language (cs.CL),FOS: Computer and information sciences},
  file = {/Users/gwbrck/Zotero/storage/NMDMDAPV/Törnberg - 2024 - Best Practices for Text Annotation with Large Language Models.pdf}
}

@article{tornbergLargeLanguageModels2025,
  title = {Large {{Language Models Outperform Expert Coders}} and {{Supervised Classifiers}} at {{Annotating Political Social Media Messages}}},
  author = {Törnberg, Petter},
  date = {2025-12},
  journaltitle = {Social Science Computer Review},
  shortjournal = {Social Science Computer Review},
  volume = {43},
  number = {6},
  pages = {1181--1195},
  issn = {0894-4393, 1552-8286},
  doi = {10.1177/08944393241286471},
  url = {https://journals.sagepub.com/doi/10.1177/08944393241286471},
  urldate = {2025-11-21},
  abstract = {Instruction-tuned Large Language Models (LLMs) have recently emerged as a powerful new tool for text analysis. As these models are capable of zero-shot annotation based on instructions written in natural language, they obviate the need of large sets of training data—and thus bring potential paradigm-shifting implications for using text as data. While the models show substantial promise, their relative performance compared to human coders and supervised models remains poorly understood and subject to significant academic debate. This paper assesses the strengths and weaknesses of popular fine-tuned AI models compared to both conventional supervised classifiers and manual annotation by experts and crowd workers. The task used is to identify the political affiliation of politicians based on a single X/Twitter message, focusing on data from 11 different countries. The paper finds that GPT-4 achieves higher accuracy than both supervised models and human coders across all languages and country contexts. In the US context, it achieves an accuracy of 0.934 and an inter-coder reliability of 0.982. Examining the cases where the models fail, the paper finds that the LLM—unlike the supervised models—correctly annotates messages that require interpretation of implicit or unspoken references, or reasoning on the basis of contextual knowledge—capacities that have traditionally been understood to be distinctly human. The paper thus contributes to our understanding of the revolutionary implications of LLMs for text analysis within the social sciences.},
  langid = {english},
  file = {/Users/gwbrck/Zotero/storage/46JA8435/Törnberg - 2025 - Large Language Models Outperform Expert Coders and Supervised Classifiers at Annotating Political So.pdf}
}

@article{spirlingWhyOpensourceGenerative2023,
  title = {Why Open-Source Generative {{AI}} Models Are an Ethical Way Forward for Science},
  author = {Spirling, Arthur},
  date = {2023-04-20},
  journaltitle = {Nature},
  shortjournal = {Nature},
  volume = {616},
  number = {7957},
  pages = {413--413},
  issn = {0028-0836, 1476-4687},
  doi = {10.1038/d41586-023-01295-4},
  url = {https://www.nature.com/articles/d41586-023-01295-4},
  urldate = {2025-11-26},
  langid = {english},
  file = {/Users/gwbrck/Zotero/storage/VBXIBXK6/Spirling - 2023 - Why open-source generative AI models are an ethical way forward for science.pdf}
}


@article{alizadehOpensourceLLMsText2025,
  title = {Open-Source {{LLMs}} for Text Annotation: A Practical Guide for Model Setting and Fine-Tuning},
  shorttitle = {Open-Source {{LLMs}} for Text Annotation},
  author = {Alizadeh, Meysam and Kubli, Maël and Samei, Zeynab and Dehghani, Shirin and Zahedivafa, Mohammadmasiha and Bermeo, Juan D. and Korobeynikova, Maria and Gilardi, Fabrizio},
  date = {2025-02},
  journaltitle = {Journal of Computational Social Science},
  shortjournal = {J Comput Soc Sc},
  volume = {8},
  number = {1},
  pages = {17},
  issn = {2432-2717, 2432-2725},
  doi = {10.1007/s42001-024-00345-9},
  url = {https://link.springer.com/10.1007/s42001-024-00345-9},
  urldate = {2025-12-01},
  abstract = {Abstract                            This paper studies the performance of open-source Large Language Models (LLMs) in text classification tasks typical for political science research. By examining tasks like stance, topic, and relevance classification, we aim to guide scholars in making informed decisions about their use of LLMs for text analysis and to establish a baseline performance benchmark that demonstrates the models’ effectiveness. Specifically, we conduct an assessment of both zero-shot and fine-tuned LLMs across a range of text annotation tasks using news articles and tweets datasets. Our analysis shows that fine-tuning improves the performance of open-source LLMs, allowing them to match or even surpass zero-shot GPT                                                   \$\$-\$\$                                        -                                                                  3.5 and GPT-4, though still lagging behind fine-tuned GPT                                                   \$\$-\$\$                                        -                                                                  3.5. We further establish that fine-tuning is preferable to few-shot training with a relatively modest quantity of annotated text. Our findings show that fine-tuned open-source LLMs can be effectively deployed in a broad spectrum of text annotation applications. We provide a Python notebook facilitating the application of LLMs in text annotation for other researchers.},
  langid = {english},
  file = {/Users/gwbrck/Zotero/storage/C3K8XDK9/Alizadeh et al. - 2025 - Open-source LLMs for text annotation a practical guide for model setting and fine-tuning.pdf}
}
